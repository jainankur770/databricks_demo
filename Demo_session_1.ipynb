{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a64ddd1-381e-4db5-888d-5ee435f5438d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Demo Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b9ff2e0-2fd2-4cd0-82ba-293482751ed5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks Notebooks have some Apache Spark variables already defined.\n",
    "# SparkContext: sc\n",
    "# SQLContext/HiveContext: sqlContext\n",
    "# SparkSession (Spark 2.x): spark\n",
    "\n",
    "print(\"Spark version\", sc.version, spark.sparkContext.version, spark.version)\n",
    "print(\"Python version\", sc.pythonVer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db8cafa6-2a37-4133-9928-b29e9c5245d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "println(sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80d03a65-db80-440d-99ce-b541019980b0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "r = requests.get(\"https://timeseries.surge.sh/usd_to_eur.csv\")\n",
    "df = spark.read.csv(sc.parallelize(r.text.splitlines()), header=True, inferSchema=True)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a840803f-6743-4008-b5b4-0122e336040b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "container = dbutils.widgets.get(\"container\")\n",
    "dbutils.jobs.taskValues.set(key = 'container', value = container)\n",
    "storageAccount = dbutils.widgets.get(\"storageAccount\")\n",
    "accessKey = dbutils.widgets.get(\"accessKey\")\n",
    "#container = \"uniqcontainer\"\n",
    "#storageAccount = \"uniq1\"\n",
    "#accessKey = \"hvyIkxc8EC1PrrYo/3uXd1xHZfu2kpfJP0wCyr4Xguien83cur9AzW5Uwm3sf4RSQmkd9IKAnV1C+AStiNKPJQ==\"\n",
    "\n",
    "accountKey = \"fs.azure.account.key.{}.blob.core.windows.net\".format(storageAccount)\n",
    "\n",
    "# Set the credentials to Spark configuration\n",
    "spark.conf.set(\n",
    "  accountKey,\n",
    "  accessKey)\n",
    "\n",
    "# Set the access key also in SparkContext to be able to access blob in RDD\n",
    "# Hadoop configuration options set using spark.conf.set(...) are not accessible via SparkContext..\n",
    "# This means that while they are visible to the DataFrame and Dataset API, they are not visible to the RDD API.\n",
    "\n",
    "spark._jsc.hadoopConfiguration().set(\n",
    "  accountKey,\n",
    "  accessKey)\n",
    "\n",
    "# Mount the drive for native python\n",
    "inputSource = \"wasbs://{}@{}.blob.core.windows.net\".format(container, storageAccount)\n",
    "mountPoint = \"/mnt/\" + container\n",
    "extraConfig = {accountKey: accessKey}\n",
    "\n",
    "print(\"Mounting: {}\".format(mountPoint))\n",
    "\n",
    "try:\n",
    "  dbutils.fs.mount(\n",
    "    source = inputSource,\n",
    "    mount_point = str(mountPoint),\n",
    "    extra_configs = extraConfig\n",
    "  )\n",
    "  print(\"=> Succeeded\")\n",
    "except Exception as e:\n",
    "  if \"Directory already mounted\" in str(e):\n",
    "    print(\"=> Directory {} already mounted\".format(mountPoint))\n",
    "  else:\n",
    "    raise(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ba11632-281b-4cf9-ae7c-d9d63c23a18b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.help()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1689e7f-8a7b-47db-9507-82ac42aa2920",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5572667d-b652-431d-8b20-fa70233767cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# using a WASB file path formatted like this:\n",
    "# wasbs://<containername>@<accountname>.blob.core.windows.net/<partialPath>\n",
    "# WASB (Windows Azure Storage Blob) is an extension built on top of the HDFS APIs. HDFS, the Hadoop Distributed File System, is one of the core Hadoop components that manage data and storage on multiple nodes.\n",
    "inputFilePath = \"wasbs://{}@{}.blob.core.windows.net/{}\".format(container, storageAccount, \"/usd_to_eur.csv\")\n",
    "df = spark.read.format(\"csv\").load(inputFilePath, header=True, inferSchema=True)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1707b905-b281-41b6-97a8-24d29a506c51",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"CiMgVXNpbmcgYSBtb3VudCBwb2ludCBvbiB3b3JrZXIgbm9kZXMgd2l0aCBEYXRhYnJpY2tzIEZTIHByb3RvY29sIGFuZCByZXF1ZXN0IGZpbGVzIHVzaW5nIGEgZmlsZSBwYXRoIGxpa2U6CiMgZGJmczovbW50Lzxjb250YWluZXJuYW1lPi88cGFydGlhbFBhdGg+CmlucHV0RmlsZVBhdGggPSAiZGJmczovbW50L3t9L3t9Ii5mb3JtYXQoY29udGFpbmVyLCAidXNkX3RvX2V1ci5jc3YiKQpkZiA9IHNwYXJrLnJlYWQuZm9ybWF0KCJjc3YiKS5sb2FkKGlucHV0RmlsZVBhdGgsIGhlYWRlcj1UcnVlLCBpbmZlclNjaGVtYT1UcnVlKQpkaXNwbGF5KGRmKQ==\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView0206954\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView0206954\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView0206954\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView0206954) SELECT `Date`,SUM(`Rate`) `column_7e940355105` FROM q GROUP BY `Date`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView0206954\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "Date",
             "id": "column_7e940355104"
            },
            "y": [
             {
              "column": "Rate",
              "id": "column_7e940355105",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "line",
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0[.]00000",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_7e940355105": {
             "name": "Rate",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "668a1126-c074-4100-ae60-824cd78dcddb",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "dc71db7f-f205-4585-8ff6-d05900b20f40",
       "origId": 385579233988596,
       "parentHierarchy": [
        "4933d04b-2b50-40eb-a701-fb18bf44b0fb"
       ],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 10.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "Date",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "Date",
           "type": "column"
          },
          {
           "alias": "column_7e940355105",
           "args": [
            {
             "column": "Rate",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"CiMgVXNpbmcgYSBtb3VudCBwb2ludCBvbiB3b3JrZXIgbm9kZXMgd2l0aCBEYXRhYnJpY2tzIEZTIHByb3RvY29sIGFuZCByZXF1ZXN0IGZpbGVzIHVzaW5nIGEgZmlsZSBwYXRoIGxpa2U6CiMgZGJmczovbW50Lzxjb250YWluZXJuYW1lPi88cGFydGlhbFBhdGg+CmlucHV0RmlsZVBhdGggPSAiZGJmczovbW50L3t9L3t9Ii5mb3JtYXQoY29udGFpbmVyLCAidXNkX3RvX2V1ci5jc3YiKQpkZiA9IHNwYXJrLnJlYWQuZm9ybWF0KCJjc3YiKS5sb2FkKGlucHV0RmlsZVBhdGgsIGhlYWRlcj1UcnVlLCBpbmZlclNjaGVtYT1UcnVlKQpkaXNwbGF5KGRmKQ==\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView54c0f0e\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView54c0f0e\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView54c0f0e\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView54c0f0e) SELECT DATE_TRUNC('YEAR',`Date`) `column_7e940355117`,SUM(`Rate`) `column_7e940355115` FROM q GROUP BY `column_7e940355117`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView54c0f0e\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 2",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "Date",
             "id": "column_7e940355117",
             "transform": "YEAR_LEVEL"
            },
            "y": [
             {
              "column": "Rate",
              "id": "column_7e940355115",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numBins": 10,
           "numberFormat": "0,0[.]00000",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_7e940355113": {
             "name": "Rate",
             "type": "column",
             "yAxis": 0
            },
            "column_7e940355115": {
             "type": "column",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "83c9ea72-307c-4058-b493-d61c402db89b",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "082f502a-57c9-421b-b483-c89963b99e3d",
       "origId": 385579233988597,
       "parentHierarchy": [
        "4933d04b-2b50-40eb-a701-fb18bf44b0fb"
       ],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 11.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "column_7e940355117",
           "type": "column"
          }
         ],
         "selects": [
          {
           "alias": "column_7e940355117",
           "args": [
            {
             "column": "Date",
             "type": "column"
            },
            {
             "string": "YEAR",
             "type": "string"
            }
           ],
           "function": "DATE_TRUNC",
           "type": "function"
          },
          {
           "alias": "column_7e940355115",
           "args": [
            {
             "column": "Rate",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Using a mount point on worker nodes with Databricks FS protocol and request files using a file path like:\n",
    "# dbfs:/mnt/<containername>/<partialPath>\n",
    "inputFilePath = \"dbfs:/mnt/{}/{}\".format(container, \"usd_to_eur.csv\")\n",
    "df = spark.read.format(\"csv\").load(inputFilePath, header=True, inferSchema=True)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c23552f-13b4-4b2b-8b3a-dce0ed8344d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8836940-914d-46ea-8afd-41e999e8caba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5df4b260-c606-4eab-847d-97bed590d6ce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a91e94e-6455-46a9-b7f3-6d974193c4a5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We can register the input dataframe as a temporary view named xrate in the SQL context\n",
    "df.createOrReplaceTempView(\"erate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bb811a3-1210-41a7-a3c1-f193f35bf130",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "e_df=spark.sql(\"select YEAR(Date) as year, COUNT(Date) as count, Mean(Rate) as mean \\\n",
    "    from erate \\\n",
    "        GROUP BY YEAR(Date) order by year DESC\")\n",
    "display(e_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c2b30ae-644a-4d17-9c32-6b42ac1b3d8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "\n",
    "retDF = (\n",
    "  df\n",
    "  .groupBy(f.year(\"Date\").alias(\"year\"))\n",
    "  .agg(f.count(\"Date\").alias(\"count\"), f.mean(\"Rate\").alias(\"mean\"))\n",
    "  .sort(f.desc(\"year\"))\n",
    ")\n",
    "\n",
    "display(retDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bd104ff-83dd-4396-b5df-d495d05e7390",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plotting\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "plots = []\n",
    "pandaData = df.toPandas().reset_index().set_index('Date')\n",
    "hd_trace = go.Scatter(x=pandaData.index, y=pandaData[\"Rate\"], name=\"Rate\")\n",
    "plots.append(hd_trace)\n",
    " \n",
    "# Plot  \n",
    "p = py.plot(plots, output_type='div')\n",
    "\n",
    "displayHTML(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfd4c59f-465e-4ba4-a17f-8e396e9bab1f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql    -- # Another way of using sql api, this should be in the first line\n",
    "SELECt * FROM erate WHERE date = \"2000-01-04\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4aeec8bc-9efa-4dd5-805a-2f132ec7185b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.select(\"Date\", \"Rate\").where(\"Date = '2000-01-01'\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c8140c2-b0b8-45e1-a5ed-9ed4e9a4287a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df_with_year = df.withColumn(\"year\", year(df[\"Date\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7263f89-4077-4e0a-8d12-a04dd987aa64",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_with_year.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bd7acc6-c8d4-451c-8ba3-68f3be845216",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "output_path = f\"wasbs://{container}@{storageAccount}.blob.core.windows.net/ankur/demo_data_process.csv\"\n",
    "df_with_year.write.mode(\"overwrite\").option(\"header\", \"true\").csv(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4c5230e-e28c-4285-b00c-a0a5b15cac40",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 385579233988598,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Demo_session_1",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
